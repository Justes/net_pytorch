{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4400e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv out: torch.Size([2, 128, 6, 6])\n",
      "alex out: torch.Size([2, 5])\n",
      "using cpu device.\n",
      "E:\\codes\\nets_pytorch E:\\codes\\nets_pytorch\\data\\flower_data\n",
      "3306\n",
      "{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n",
      "{0: 'daisy', 1: 'dandelion', 2: 'roses', 3: 'sunflowers', 4: 'tulips'}\n",
      "Using 8 dataloader workers every process\n",
      "using 3306 images for training, 364 images for validation.\n",
      "conv out: torch.Size([2, 128, 6, 6])\n",
      "train epoch[1/10] loss:1.202: 100%|██████████████████████████████████████████████████| 104/104 [02:25<00:00,  1.40s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:10<00:00,  1.19it/s]\n",
      "[epoch 1] train_loss: 1.456  val_accuracy: 0.434\n",
      "train epoch[2/10] loss:1.359: 100%|██████████████████████████████████████████████████| 104/104 [02:05<00:00,  1.21s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:10<00:00,  1.16it/s]\n",
      "[epoch 2] train_loss: 1.264  val_accuracy: 0.497\n",
      "train epoch[3/10] loss:1.399: 100%|██████████████████████████████████████████████████| 104/104 [02:05<00:00,  1.21s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:09<00:00,  1.22it/s]\n",
      "[epoch 3] train_loss: 1.217  val_accuracy: 0.514\n",
      "train epoch[4/10] loss:1.272: 100%|██████████████████████████████████████████████████| 104/104 [02:08<00:00,  1.23s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.01it/s]\n",
      "[epoch 4] train_loss: 1.157  val_accuracy: 0.547\n",
      "train epoch[5/10] loss:1.232: 100%|██████████████████████████████████████████████████| 104/104 [02:06<00:00,  1.22s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:09<00:00,  1.21it/s]\n",
      "[epoch 5] train_loss: 1.095  val_accuracy: 0.478\n",
      "train epoch[6/10] loss:0.938: 100%|██████████████████████████████████████████████████| 104/104 [02:02<00:00,  1.18s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:10<00:00,  1.10it/s]\n",
      "[epoch 6] train_loss: 1.086  val_accuracy: 0.475\n",
      "train epoch[7/10] loss:1.009: 100%|██████████████████████████████████████████████████| 104/104 [02:18<00:00,  1.34s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.05it/s]\n",
      "[epoch 7] train_loss: 1.050  val_accuracy: 0.549\n",
      "train epoch[8/10] loss:1.192: 100%|██████████████████████████████████████████████████| 104/104 [02:13<00:00,  1.29s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:10<00:00,  1.13it/s]\n",
      "[epoch 8] train_loss: 1.044  val_accuracy: 0.588\n",
      "train epoch[9/10] loss:0.640: 100%|██████████████████████████████████████████████████| 104/104 [02:24<00:00,  1.39s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:09<00:00,  1.23it/s]\n",
      "[epoch 9] train_loss: 1.040  val_accuracy: 0.624\n",
      "train epoch[10/10] loss:0.881: 100%|█████████████████████████████████████████████████| 104/104 [02:17<00:00,  1.33s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:09<00:00,  1.24it/s]\n",
      "[epoch 10] train_loss: 1.008  val_accuracy: 0.571\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "%run AlexNet.ipynb\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"using {} device.\".format(device))\n",
    "    \n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ]),\n",
    "        \"val\": transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "    }\n",
    "    \n",
    "    data_root = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "    image_path = os.path.join(data_root, \"data\", \"flower_data\")\n",
    "    assert os.path.exists(image_path),\"{} path does not exists.\".format(image_path)\n",
    "    print(data_root, image_path)\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "    print(train_num)\n",
    "    \n",
    "    flower_list = train_dataset.class_to_idx\n",
    "    print(flower_list)\n",
    "    cla_dict = dict((val, key) for key, val in flower_list.items())\n",
    "    print(cla_dict)\n",
    "    \n",
    "    json_str = json.dumps(cla_dict, indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "        \n",
    "    batch_size = 32\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, 'val'), transform=data_transform['val'])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=nw)\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = AlexNet()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "\n",
    "    epochs = 10\n",
    "    save_path = './AlexNet.pth'\n",
    "    best_acc = 0.0\n",
    "    train_steps = len(train_loader)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()           \n",
    "            outputs = net(images.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            running_loss += loss.item()            \n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "        \n",
    "        net.eval()\n",
    "        acc = 0.0\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(validate_loader, file=sys.stdout)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = net(val_images.to(device))\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "            \n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "        \n",
    "        if val_accurate > best_acc :\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "            \n",
    "    print(\"Finished Training\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf65adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
